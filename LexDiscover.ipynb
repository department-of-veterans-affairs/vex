{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @author Alec Chapman\n",
    "# @date 1/19/2018\n",
    "# @author Olga Patterson\n",
    "# @date 10/01/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import LexDiscover as ld\n",
    "from gensim.models import Word2Vec\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is meant to be used as a simple interface for using the `LexDiscover` toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Add Data\n",
    "\n",
    "Point to the data.\n",
    "\n",
    "### Option A: Read from a file/directory\n",
    "This can be either a directory, in which case all .txt files will be read in, or a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH: Change this to a path to either a single file or a directory containing .txt files\n",
    "# If it's a directory, all .txt files in that directory will be read in\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "text = read_text_from_file(DATA_PATH, lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Read from a SQL database\n",
    "Needs a database name, an SQL query to complete, and the column name that has the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database = ''\n",
    "query = ''\n",
    "col_name = 'text'\n",
    "\n",
    "text = read_SQL_server(database=database, query=query, col_name=col_name, lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Base Lexicon of Terms and SNOMED-CT codes.\n",
    "These are the terms and SNOMED-CT codes that will be used for seeding the algorithms. You can either:\n",
    "1. Type them in manually going to the the line that says `# BASE_LEX = []` and typing comma-separated terms between the brackets enclosed by double-quotes (ie., `BASE_LEX = [\"cardiac arrest\", \"heart attack\"]`)\n",
    "2. Uncomment the line that says `# BASE_LEX = read_base_lex(path='lex.txt', sep='\\n')`. Change to the location of your lexicon. This will read in your lexicon from a file. If the terms are separated by commas, change to `sep=','`. Default is new lines.\n",
    "\n",
    "For now, SNOMED-CT can only be utilized by entering codes as ints. I suggest you find some and copy and paste them from [the SNOMED browser](http://browser.ihtsdotools.org/?). Future support could automatically map terms in the base lexicon to SNOMED-CT codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LEX = []\n",
    "BASE_LEX = [\"cardiac arrest\", \"heart attack\"]\n",
    "#BASE_LEX = read_base_lex(path='example_lex.txt', sep='\\n')\n",
    "\n",
    "BASE_CODES = []\n",
    "BASE_CODES=[56265001,194828000, 410429000]  # Heart disease, Cardiac arrest, Angina (disorder)\n",
    "#BASE_CODES = read_base_codes(path='example_codes.txt', sep='\\n')\n",
    "\n",
    "print(BASE_LEX)\n",
    "print(BASE_CODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Configurations\n",
    "\n",
    "Set the configurations for the models, or leave the values as is to be default. Here is a description of each parameter:\n",
    "- `MIN_COUNT`: the number of times a word must occur to be considered. A higher number will yield more precise results. Default is 1 (any word in the corpus is considered).\n",
    "- `SIM_THRESH`: the minimum similarity between two words to be added by the machine learning algorithm. A higher threshold will yield more precise results. Default is 0.5.\n",
    "- `EDIT_DIST`: the number of steps of edits for the linguistic algorithm to generate misspellings. A lower number will yield more precise results. Default is 2.\n",
    "- `PARENTS`: whether to search for synonyms of parent concepts in SNOMED-CT. Default `True`.\n",
    "- `CHILDREN`: whether to search for synonyms of child concepts in SNOMED-CT. Default `True`.\n",
    "- `MODELS`: which models you want to use.\n",
    "    - `'word2vec'`: a machine learnin model that will find new terms using vector similarity. Slightly noisy but has the potential to come up with good words.\n",
    "    - `'ont'`: uses SNOMED codes to find parent/childrent concepts. The most likely to find synonyms but will not find misspellings or abbreviations.\n",
    "    - `'wnling'`: linguistic variations, finds misspellings and abbreviations. This is likely to be noisy and may find words that are completely unrelated, but it can also find good terms that aren't in normal vocabulries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 1\n",
    "SIM_THRESH = 0.5\n",
    "EDIT_DIST = 2\n",
    "PARENTS = True\n",
    "CHILDREN = True\n",
    "\n",
    "# Delete any strings you don't want to use\n",
    "MODELS = ['word2vec', 'ont', 'wnling']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Initiate model and run discovery algorithm\n",
    "This will return a list, `new_lex`, that contains all new words that have been added to the vocabulary. You can then save the expanded lexicon to a file using the next cell with `write_lex(filename)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(\"word2vec.model\")\n",
    "MODELS={}\n",
    "MODELS['word2vec'] = w2v\n",
    "print(\"Loaded word2vec model: \", w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a minute, especially if a large lexicon was passed in.\n",
    "discover = ld.AggregateLexDiscover(\n",
    "    text=text,\n",
    "    base_lex=BASE_LEX,\n",
    "    base_codes=BASE_CODES,\n",
    "    min_count=MIN_COUNT,\n",
    "    edit_dist=EDIT_DIST,\n",
    "    sim_thresh=SIM_THRESH,\n",
    "    models=MODELS,\n",
    "    parents=PARENTS,\n",
    "    children=CHILDREN,\n",
    ")\n",
    "\n",
    "new_lex = discover.discover_lex()\n",
    "print(\"{} new terms added to lexicon\".format(len(new_lex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results to file\n",
    "discover.write_lex('expanded_lex.txt', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Explore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort new terms by similarity\n",
    "discover.sort_by_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all new terms and their frequency in the corpus\n",
    "for term in discover.get_discovered_terms():\n",
    "    print(term, discover.get_count(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look only at words that were discovered using machine learning\n",
    "for term in discover.get_discovered_terms('word2vec'):\n",
    "    print(term, discover.get_count(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look only at words that were discovered using linguistic variations\n",
    "for term in discover.get_discovered_terms('wnling'):\n",
    "    print(term, discover.get_count(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look only at words that were discovered using SNOMED\n",
    "for term in discover.get_discovered_terms('ont'):\n",
    "    print(term, discover.get_count(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at words that were discovered by two models\n",
    "for term in discover.get_intersect('wnling', 'word2vec'):\n",
    "    print(term, discover.get_count(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Or all three\n",
    "for term in discover.get_intersect('word2vec', 'wnling','ont'):\n",
    "    print(term, discover.get_count(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the contexts of a term (surrounding words)\n",
    "# Prints contexts and probabilities (frequency of context/frequency of term)\n",
    "# 'PHI' = before sentence\n",
    "# 'OMEGA' = after sentence\n",
    "term = 'angina'\n",
    "num_words_before = 1\n",
    "num_words_after = 0\n",
    "remove_stopwords = True\n",
    "\n",
    "window = (num_words_before, num_words_after)\n",
    "for context in discover.get_context(term, window=window, remove_stopwords=remove_stopwords):\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all sentences containing a word\n",
    "term_to_find = 'cardiovascular'\n",
    "num_sentences_to_see = 10\n",
    "\n",
    "g = discover.search_in_sentences(term_to_find, num_sentences_to_see)\n",
    "for s in g:\n",
    "    print ('\\n',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
