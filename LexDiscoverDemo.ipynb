{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @author Alec Chapman\n",
    "# @date 1/19/2018\n",
    "# @author Olga Patterson\n",
    "# @date 10/01/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import LexDiscover as ld\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from importlib import reload\n",
    "reload(ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "Code repository: https://github.com/department-of-veterans-affairs/vex\n",
    "\n",
    "Documentation: https://github.com/department-of-veterans-affairs/vex/wiki\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is meant to be an in-depth introduction to the `LexDiscover` module. Another notebook will be provided, generically titled `LexDiscover.ipynb`, that will be meant as a plug-and-play notebook with limited functionality.\n",
    "\n",
    "**Outline**\n",
    "- Motivation\n",
    "- Overview\n",
    "- Example\n",
    "    - Set Up\n",
    "    - Example\n",
    "- Exploring the Results\n",
    "    - Getting the lexicon\n",
    "    - Set operations\n",
    "    - Getting contexts\n",
    "- Next Steps\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "Rule-based NLP systems require an extensive lexicon in order to discover concepts in clinical text. Depending on the task, this lexicon may be relatively simple to develop. However, challenges in developing a lexicon resulting from the varying nature of clinical text includes misspellings, abbreviations, and little-known synonyms. Discovering additional representations of these terms can increase an NLP system's coverage and speed up the development process.\n",
    "\n",
    "This module is meant to help discover new lexical variants of any concept. It is not meant to be an automated part of a pipeline, but rather a toolkit to help the system developer come up with an extensive lexicon. This module is heavily inspired by the following paper by Velupillai et al.: [\"Vocabulary Development To Support Information Extraction of Substance Abuse from Psychiatry Notes\"](http://www.aclweb.org/anthology/W/W16/W16-2912.pdf) [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "As outlined in the paper by Velupillai et al., we implement vocabulary expansion using a number of different sources. Currently, three methods are implemented: an automated, neural-network based method using the `word2vec` implementation by `GenSim` , called `\"word2vec\"`;  a linguistic and rule-based implementation called `wnling`; and an ontological-based implementation called `ont`. \n",
    "\n",
    "The user provides a corpus of text and a base lexicon to generate new vocabulary terms. Each word in the base lexicon is then used to expand the vocabulary by finding similar words in the corpus. The corpus should ideally be the same corpus that will be used for the NLP task; for example, if the study involves extracting information from radiology reports, the corpus should consist of radiology reports similar or identical those that will be processed in the study. This insures that similar words will be found and returned. Any discovered words that occur in the corpus above a user-defined minimum count are kept.\n",
    "\n",
    "The `word2vec` algorithm uses the context of words in a vocabulary to create dense, real-valued vectors representing words [2]. We use the GenSim implementation [3]. The resulting vectors can be used to measure similarity between words. In this implementation, each word in the corpus is queried against each word the base lexicon. Any words that have a similarity score above a user-defined threshold is added to the lexicon. This does not necessarily return synonyms; it may return antonyms or other words that appear in similar contexts. As such, the results of the `word2vec` method may be especially noisy and should be used with caution, but may still discover useful terms. \n",
    "\n",
    "`wnling` uses string manipulations and orthographic rules to generate new terms. Currently, new strings are generated using substuutition, replacement deletion, and abbreviation. For example, \"abdomen\" becomes \"abdomin\", \"abdomi\", \"abd\", etc. \"Cardiac arrest\" becomes \"crdc arrst\", \"ca\", and \"car arr\". Generated strings that do not occur in the corpus above the min_count threshold are discarded.\n",
    "\n",
    "`ont` uses SNOMED-CT codes to find synonyms, parents, and children of concepts. The user provides base codes and all terms belonging to that code, and optionally parents and children, are searched for in the corpus and added.\n",
    "\n",
    "Future implementations should include additional linguistic generations (inflection, term reordering) and more expansive ontological-based expansion (UMLS, text searching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To instantiate our vocabulary discovery model, we'll need to pass in the corpus as a single string. Any preprocessing can be done beforehand; by default, the only preprocessing will be lower-casing and replacing multi-word phrases from the base lexicon with single words joined by underscored (all occurrences in the corpus of \"cardiac arrest\" become \"cardiac_arrest\"). \n",
    "\n",
    "In this example, we'll pass in a directory containing text files that you want to read in (or change the code below to be the path to a single file.) We'll then read all the files in and train a word2vec model which we'll use to pass in to our discovery objects. Future functionality should include methods to read from files/directories or database clients.\n",
    "\n",
    "The texts in this example are radiology reports from MIMIC-III [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data here using helper functions\n",
    "# Pass into discover as a single string.\n",
    "import helpers\n",
    "\n",
    "DATADIR = 'data' # directory containing *.txt files\n",
    "text = helpers.read_text_from_file(DATADIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "We first define our base lexicon: a list of pre-selected words that will be used to define our concept. In this example, we'll assume that we are aiming to extract anatomic locations from radiology reports. This is a broad task and is well-suited for this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_lex = ['diabetes', 'depression', 'hypertension']\n",
    "base_codes = [73211009, 38341003] # Diabetes mellitus,  Diabetes mellitus type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've provided some initial examples, we can initialize our model. The main class to be used is `LexDiscover.AggregateLexDiscover`, which will utilize all of implemented methods. We pass in the following keyword arguments:\n",
    "\n",
    "- `text`: a corpus of clinical text as a single string\n",
    "- `base_lex`: a list of intial terms\n",
    "- `min_count`: the minimum number of times a word must occur to be added to the lexicon. Default is 1, so any word occurring in the corpus will be considered.\n",
    "- `edit_dist`: the number of substitutions, replacements, and deletions that will occur in `wnling` (think Minimum Edit Distance). Default is 2.\n",
    "- `sim_thresh`: the similarity threshold that `word2vec` will use to decide whether or not to add a new term to the lexicon. Default is 0.5\n",
    "- `base_codes`: a list of initial SNOMED-CT codes\n",
    "- `children`: Boolean, whether or not to expand to a base code's children. Default False.\n",
    "- `parents`: Boolean, whether or not to expand to a base code's parents. Default False.\n",
    "\n",
    "Upon initializing, the model will create a new dictionary containing the current lexicon, lower-case the text, replace any multi-word phrases in the text with single strings, split the corpus up into sentences, and then initialize the sub-models. This will automatically train a `word2vec` model using the default parameters. Alternatively, the sub-models can be passed in directly (more on that in the future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import LexDiscover as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing this model takes some time due to the text processing and model training.\n",
    "discover = ld.AggregateLexDiscover(text=text, base_lex=base_lex, min_count=1, edit_dist=2, sim_thresh=0.5,\n",
    "                                  base_codes=base_codes, children=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(\"word2vec.model\")\n",
    "existing_models={}\n",
    "existing_models['word2vec'] = w2v\n",
    "print(\"Loaded word2vec model: \", w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discover = ld.AggregateLexDiscover(text=text, base_lex=base_lex, min_count=1, edit_dist=2, \n",
    "                                   sim_thresh=0.5,\n",
    "                                   models=existing_models,\n",
    "                                   base_codes=base_codes, \n",
    "                                   children=True, \n",
    "                                   parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discover new lexical terms, we call `discover.discover_lex()`. This will call the same method on the sub-models and discover new lexical terms using each method. This method returns a list of new words sorted by frequency (most frequent words at the top), as well as adds each new term to the lexicon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_terms = discover.discover_lex()\n",
    "print(discover)\n",
    "print()\n",
    "print(new_terms[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a number of useful words were retrieved: \"pelvis\", \"chest\", and \"bladder\" are all anatomic locations similar to those found in the base lexicon. Additionally, \"abd\", an abbreviation for \"abdomen\", was retrieved. Some words are not necessarily useful in themselves, but are part of multi-word phrases: \"anterior\" and \"lower\" are all likely part of multi-word phrases that should be considered. More support for multi-word phrases should be added in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some words are not useful, such as \"contrast\" and \"terminates\". These are words that are commonly mentioned *in the same context* as the anatomic locations in radiology reports, so they were retrieved by `word2vec`. We'll next examine more closely the results of the different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-models\n",
    "The sub-models can be directly accessed through `discover`'s `model` attribute. Instead of using the class `AggregateLexDiscover`, each of these models can be instantiated separately to only use one of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(discover.models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `word2vec` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at which words were retrieved by `word2vec`. We can do this by calling the `get_discovered_terms` and passing in the model name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_terms = discover.get_discovered_terms('word2vec')\n",
    "print(len(word2vec_terms))\n",
    "print(word2vec_terms[:40]) # sorted by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of new terms were discovered by `word2vec`. As said before, this method is quite noisy, but depending on the task, lots of useful results can come from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `wnling` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linguistically generated words are easier to understand and justify. Let's look at the words that were generated by `wnling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_terms = discover.get_discovered_terms('wnling')\n",
    "print(len(ling_terms))\n",
    "print(ling_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this list is much smaller. Especially useful terms include 'ruq', short for 'right upper quadrant', one of our base terms, 'luq', and many misspellings of 'abdomen'. Some are not useful, like 'lie', 'fever', and 'liters'. This list is sorted by frequency in descending order (most-frequent to least-frequent). Let's look at the frequency of these by calling `discover.get_count(term)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print([(term, discover.get_count(term)) for term in ling_terms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the least common terms only occurred once, so they would be excluded by a stricter count threshold. But 'abd', 'kidneys', 'ruq', 'luq', and 'abdomenal' all occurred a fair number oftimes, suggesting that they may be useful terms to keep. Of course, the count threshold should be adjusted depending on the size of the corpus and desired precision; Velupillai et al. use a count threshold of 15 with a corpus of 100 notes, but use a baseline lexicon of 91 terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ont` model\n",
    "The ontological model using SNOMED-CT to find synonyms for concepts defined by the codes in `base_codes`. By default, it will not look for parent or children terms, but setting this keyword args to `True` can be quite useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discover.get_discovered_terms('ont')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the list above, you can see the two original terms that were added by the codes: 'kidney structure (body structure)' and  'gastrointestinal tract structure (body structure)'. Neither of these phrases actually appear in the text due to the '(body structure)' string. Here's a quick look into `pymedtermino`. This library takes some manual building in order to have access to the SNOMED-CT data, so if the installation is not complete it may cause some errors. If an `OperationalError` is raised, an empty list is returned. Check out [pymedtermino's documentation](http://pythonhosted.org/PyMedTermino/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymedtermino.snomedct import SNOMEDCT\n",
    "for code in [73211009, 44054006]:\n",
    "    concept = SNOMEDCT[code]\n",
    "    print(\"Original concept: {}\".format(concept))\n",
    "    print(\"Original synonyms: {}\".format(concept.terms))\n",
    "    print(\"1st Child synonyms:\")\n",
    "    for child in concept.children[:1]:\n",
    "        print(child.terms)\n",
    "    print(\"1st Parent synonyms:\")\n",
    "    for parent in concept.parents[:1]:\n",
    "        print(parent.terms)\n",
    "    print()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Results\n",
    "For data permanence, the entire new lexicon can be written to a file using the `write_lex` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discover.write_lex('example_lex.txt', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should then be reviewed and irrelevant terms should be discarded. You could then potentially repeat the process with the expanded vocabulary. Here are some methods useful for exploring and evaluating the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all lexical terms\n",
    "lex = discover.get_lex()\n",
    "print(len(lex))\n",
    "print(lex[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the original baseline lexicon\n",
    "base = discover.get_base_lex()\n",
    "print(len(base))\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SNOMED-CT codes. Returns False if there is no ontological model\n",
    "codes = discover.get_codes()\n",
    "print(len(codes))\n",
    "print(codes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the baseline SNOMED-CT codes\n",
    "base_codes = discover.get_base_codes()\n",
    "print(len(base_codes))\n",
    "print(base_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operations\n",
    "`get_lex()` returns a list of *all* lexicon terms, including the baseline terms. There are several methods that allow you to see just the terms that were discovered by a particular model and not in the baseline vocabulary (results are always returned sorted by frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all new terms that were not in the baseline vocabulary.\n",
    "t = discover.get_discovered_terms()\n",
    "print(len(t))\n",
    "print(t[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all terms that were discovered by a particular model\n",
    "t = discover.get_discovered_terms(\"ont\")\n",
    "print(len(t))\n",
    "print(t[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all new terms that were discovered by all of two or more models (set intersection)\n",
    "i = discover.get_intersect('word2vec', 'wnling')\n",
    "print(len(i))\n",
    "print(i[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all new terms that were discovered by any of two or more models (set union)\n",
    "u = discover.get_union('wnling', 'ont')\n",
    "print(len(u))\n",
    "print(u[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all new terms that were discovered by the first model, but not one or more others (set difference)\n",
    "# ie., only terms discovered by 'ont'\n",
    "d = discover.get_difference('ont', 'word2vec', 'wnling')\n",
    "print(len(d))\n",
    "print(d[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting contexts\n",
    "These methods, particularly `word2vec`, will return terms that are ambiguous in their usefulness, or only part of a larger, more useful phrase. To help recognize this, there are several methods that allow you to see the context of a term.\n",
    "\n",
    "`get_context(word, window=(1, 0), remove_stopwords=True)` returns a list of n-grams for a given word and their probability as a decimal, with `n` defined by the tuple `window`. For example, with a context window of (1, 0) for the term 'tract', the tuple (('sinus', 'tract'), 0.11) would mean that ~11% of instances of 'tract' are in the context of 'sinus tract'.\n",
    "\n",
    "By default, it will return context windows of one word before, 0 after (bigrams). The keyword argument `remove_stopwords` defines whether or not stopwords should be included in the windows. 'PHI' means before a sentence (**Φ**, not protected-health-information); 'OMEGA' means after a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all bigrams with stopwords removed\n",
    "c = discover.get_context('heart')\n",
    "print(len(c))\n",
    "print(c[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all words that follow 'anterior'\n",
    "c = discover.get_context('depression', window=(0, 1))\n",
    "print(len(c))\n",
    "print(c[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a larger context window, with a multi-word phrase, keeping stopwords\n",
    "c = discover.get_context('mellitus', window=(3, 6), remove_stopwords=False)\n",
    "print(len(c))\n",
    "print(c[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentences with a word in it\n",
    "# Returns a list of sentences\n",
    "g = discover.search_in_sentences('parkinson', 5)\n",
    "for i, s in enumerate(g):\n",
    "    print(i, '.', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = discover.search_in_sentences('acetaminophen' , 5)\n",
    "for i, s in enumerate(g):\n",
    "    print(i, '.', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps (1/24/18)\n",
    "- **Multi-word phrases:** Many of the retrieved words are likely just part of a multi-word phrase. Approaches to this could include simple expanding the words in the corpus to be both bigrams and unigrams; using GenSim's Phraser module to automatically combine multi-word phrases; and adding functionality to see the context of words in the corpus when reviewing results.\n",
    "- **Evaluation:** There has been little evaluation in this methodology. Evaluating the various methods of vocabulary expansion could lead to better resutls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Velupillai et al., [\"Vocabulary Development To Support Information Extraction of Substance Abuse from Psychiatry Notes\"](http://www.aclweb.org/anthology/W/W16/W16-2912.pdf)\n",
    "\n",
    "[2] Mikolov et al., [\"Distributed Representations of Words and Phrases and their Compositionality\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "[3] [GenSim word2vec documentation](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "[4] [MIMIC](https://mimic.physionet.org/)\n",
    "\n",
    "[5] [PyMed Termino](http://pythonhosted.org/PyMedTermino/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
